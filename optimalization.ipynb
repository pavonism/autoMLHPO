{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane zbiory danych: \n",
    "- Abalone https://www.openml.org/search?type=data&status=active&id=720\n",
    "- ada_prior https://www.openml.org/search?type=data&status=active&id=1037\n",
    "- spambase https://www.openml.org/search?type=data&status=active&id=44\n",
    "- phoneme https://www.openml.org/search?type=data&status=active&id=1489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane algorytmy:\n",
    "* GradientBoosting\n",
    "* RandomForest\n",
    "* Sieci neuronowe\n",
    "\n",
    "W przypadku sieci neuronowych startową siatkę hiperparametrów zaczerpnięto z artykułu https://www.degruyter.com/document/doi/10.1515/comp-2020-0227/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie mapy dataset-id oraz siatek hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {\n",
    "    # 4.17k\n",
    "    'abalone' : {\n",
    "        'id' : 720,\n",
    "    },\n",
    "    # 4.56k\n",
    "    'ada_prior' : {\n",
    "        'id' : 1037,\n",
    "    },\n",
    "    # 5.4k\n",
    "    'phoneme' : {\n",
    "        'id' : 1489,\n",
    "    },\n",
    "    # 4.6k\n",
    "    'spambase' : {\n",
    "        'id' : 44,\n",
    "    },\n",
    "}\n",
    "\n",
    "search_spaces = {\n",
    "    'random_forest' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 2000),\n",
    "        \"estimator__max_depth\" : np.arange(1, 20),\n",
    "        \"estimator__max_samples\" : np.linspace(0.1, 1, num=100),\n",
    "        \"estimator__max_features\" : ['sqrt', 'log2'],\n",
    "        \"estimator__min_samples_split\" :  np.linspace(0.1, 0.5, num=100),\n",
    "    },\n",
    "    'neural_network' : {\n",
    "        \"estimator__hidden_layer_sizes\" : np.arange(10, 500),\n",
    "        \"estimator__activation\" : ['relu', 'identity', 'logistic', 'tanh'],\n",
    "        \"estimator__learning_rate_init\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "        \"estimator__alpha\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "    },\n",
    "    'gradient_boosting' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 5000, step=50),\n",
    "        \"estimator__learning_rate\" : np.logspace(-10, 0, base=2.0, num=10),\n",
    "        \"estimator__subsample\" : np.linspace(0.1, 1, num=10),\n",
    "        \"estimator__loss\" : ['log_loss', 'exponential'],\n",
    "        \"estimator__max_depth\" : np.arange(1, 16, step=2),\n",
    "        \"estimator__min_samples_split\" : [2**(i+1) for i in range(7)],\n",
    "        \"estimator__max_features\" : np.arange(0.05, 1.05, 0.05),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPO():\n",
    "    def __init__(self, estimator, search_space, random_state=0, test_size=0.2, n_iter=10, n_jobs=1): \n",
    "        num_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='mean')),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ])\n",
    "\n",
    "        cat_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ])\n",
    "\n",
    "        preprocessing = ColumnTransformer(transformers=[\n",
    "            ('num_pipeline',num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "            ('cat_pipeline',cat_pipeline, make_column_selector(dtype_include=np.object_))\n",
    "            ],\n",
    "            remainder='drop',\n",
    "            n_jobs=-1)\n",
    "\n",
    "        self.pipeline = Pipeline(steps=[\n",
    "            ('preprocessing', preprocessing),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.search_space = search_space\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.n_jobs = n_jobs\n",
    "        self.estimator_name = estimator.__class__.__name__\n",
    "\n",
    "    def load_dataset(self, data_set):\n",
    "        dataset = openml.datasets.get_dataset(data_set['id'])\n",
    "        self.dataset_id = data_set['id']\n",
    "\n",
    "        X, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "\n",
    "        y = X.iloc[:,-1]\n",
    "        X = X.iloc[:,:-1]\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "\n",
    "    def save_to_file(self, type, results):\n",
    "        csv_file_path = '{0}-{1}-{2}-{3}.csv'.format(type, self.dataset_id, self.estimator_name, self.random_state)\n",
    "\n",
    "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "            all_param_names = set()\n",
    "            for params_dict in results['params']:\n",
    "                all_param_names.update(params_dict.keys())\n",
    "\n",
    "            fieldnames = ['Iteracja'] + list(all_param_names) + ['Srednia dokladnosc', 'Czas']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "            writer.writeheader()\n",
    "\n",
    "            for i in range(len(results['params'])):\n",
    "                row_data = {'Iteracja': i + 1}\n",
    "                row_data.update(results['params'][i])\n",
    "                row_data.update({\n",
    "                    'Srednia dokladnosc': results['mean_test_score'][i],\n",
    "                    'Czas': results['mean_fit_time'][i]\n",
    "                })\n",
    "\n",
    "                writer.writerow(row_data)\n",
    "\n",
    "        print(f\"Wyniki zostały zapisane do pliku CSV: {csv_file_path}\")\n",
    "\n",
    "    def run_random_search(self):\n",
    "        rs = RandomizedSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state, n_jobs=self.n_jobs)\n",
    "        rs.fit(self.X_train, self.y_train)\n",
    "        score = rs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('random_search', rs.cv_results_)\n",
    "\n",
    "\n",
    "    def run_bayes_search(self):\n",
    "        bs = BayesSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state, n_jobs=self.n_jobs)\n",
    "        bs.fit(self.X_train, self.y_train)\n",
    "        score = bs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('bayes_search', bs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    algorithms = [\n",
    "        ('random_forest', RandomForestClassifier()),\n",
    "        ('neural_network', MLPClassifier()),\n",
    "        ('gradient_boosting', GradientBoostingClassifier())\n",
    "    ]\n",
    "\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        hpo = HPO(algorithm[1], search_spaces[algorithm[0]], n_iter=500, n_jobs=-1)\n",
    "        for data_set in data_sets:\n",
    "            hpo.load_dataset(data_sets[data_set])\n",
    "            hpo.run_random_search()\n",
    "            hpo.run_bayes_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
