{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane zbiory danych: \n",
    "- Abalone https://www.openml.org/search?type=data&status=active&id=720\n",
    "- ada_prior https://www.openml.org/search?type=data&status=active&id=1037\n",
    "- spambase https://www.openml.org/search?type=data&status=active&id=44\n",
    "- phoneme https://www.openml.org/search?type=data&status=active&id=1489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane algorytmy:\n",
    "* GradientBoosting\n",
    "* RandomForest\n",
    "* Sieci neuronowe\n",
    "\n",
    "W przypadku sieci neuronowych startową siatkę hiperparametrów zaczerpnięto z artykułu https://www.degruyter.com/document/doi/10.1515/comp-2020-0227/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "1. Sprawdzić dane - done\n",
    "2. Uporządkować metody w ramach jednej klasy - done\n",
    "3. Zapisywanie wyników - done\n",
    "4. Wybranie siatek hiperparamterów - done\n",
    "5. Obliczanie tuningu \n",
    "6. Wykresiki i docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_sets = {\n",
    "    # 4.17k\n",
    "    'abalone' : {\n",
    "        'id' : 720,\n",
    "        'label_y' : 'target'\n",
    "    },\n",
    "    # 4.56k\n",
    "    'ada_prior' : {\n",
    "        'id' : 1037,\n",
    "        'label_y' : 'target'\n",
    "    },\n",
    "    # 5.4k\n",
    "    'phoneme' : {\n",
    "        'id' : 1489,\n",
    "        'label_y' : 'class'\n",
    "    },\n",
    "    # 4.6k\n",
    "    'spambase' : {\n",
    "        'id' : 44,\n",
    "        'label_y' : 'class'\n",
    "    },\n",
    "}\n",
    "\n",
    "search_spaces = {\n",
    "    'random_forest' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 2000),\n",
    "        \"estimator__max_depth\" : np.arange(1, 20),\n",
    "        \"estimator__max_samples\" : np.linspace(0.1, 1, num=100),\n",
    "        \"estimator__max_features\" : ['sqrt', 'log2'],\n",
    "        \"estimator__min_samples_split\" :  np.linspace(0.1, 0.5, num=100),\n",
    "    },\n",
    "    'neural_network' : {\n",
    "        \"estimator__hidden_layer_sizes\" : np.arange(10, 500),\n",
    "        \"estimator__activation\" : ['relu', 'identity', 'logistic', 'tanh'],\n",
    "        \"estimator__learning_rate_init\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "        \"estimator__alpha\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "    },\n",
    "    'gradient_boosting' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 5000, step=50),\n",
    "        \"estimator__learning_rate\" : np.logspace(-10, 0, base=2.0, num=10),\n",
    "        \"estimator__subsample\" : np.linspace(0.1, 1, num=10),\n",
    "        \"estimator__loss\" : ['log_loss', 'exponential'],\n",
    "        \"estimator__max_depth\" : np.arange(1, 16, step=2),\n",
    "        \"estimator__min_samples_split\" : [2**(i+1) for i in range(7)],\n",
    "        \"estimator__max_features\" : np.arange(0.05, 1.05, 0.05),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "class HPO():\n",
    "    def __init__(self, estimator, search_space, random_state=0, test_size=0.2, n_iter=10, n_jobs=1): \n",
    "        num_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='mean')),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ])\n",
    "\n",
    "        cat_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ])\n",
    "\n",
    "        preprocessing = ColumnTransformer(transformers=[\n",
    "            ('num_pipeline',num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "            ('cat_pipeline',cat_pipeline, make_column_selector(dtype_include=np.object_))\n",
    "            ],\n",
    "            remainder='drop',\n",
    "            n_jobs=-1)\n",
    "\n",
    "        self.pipeline = Pipeline(steps=[\n",
    "            ('preprocessing', preprocessing),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.search_space = search_space\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.n_jobs = n_jobs\n",
    "        self.estimator_name = estimator.__class__.__name__\n",
    "\n",
    "    def load_dataset(self, data_set):\n",
    "        dataset = openml.datasets.get_dataset(data_set['id'])\n",
    "        self.dataset_id = data_set['id']\n",
    "\n",
    "        X, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "\n",
    "        y = X.iloc[:,-1]\n",
    "        X = X.iloc[:,:-1]\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "\n",
    "    def save_to_file(self, type, results):\n",
    "        csv_file_path = '{0}-{1}-{2}-{3}.csv'.format(type, self.dataset_id, self.estimator_name, self.random_state)\n",
    "\n",
    "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "            all_param_names = set()\n",
    "            for params_dict in results['params']:\n",
    "                all_param_names.update(params_dict.keys())\n",
    "\n",
    "            fieldnames = ['Iteracja'] + list(all_param_names) + ['Srednia dokladnosc', 'Czas']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "            writer.writeheader()\n",
    "\n",
    "            for i in range(len(results['params'])):\n",
    "                row_data = {'Iteracja': i + 1}\n",
    "                row_data.update(results['params'][i])\n",
    "                row_data.update({\n",
    "                    'Srednia dokladnosc': results['mean_test_score'][i],\n",
    "                    'Czas': results['mean_fit_time'][i]\n",
    "                })\n",
    "\n",
    "                writer.writerow(row_data)\n",
    "\n",
    "        print(f\"Wyniki zostały zapisane do pliku CSV: {csv_file_path}\")\n",
    "\n",
    "    def run_random_search(self):\n",
    "        rs = RandomizedSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state, n_jobs=self.n_jobs)\n",
    "        rs.fit(self.X_train, self.y_train)\n",
    "        score = rs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('random_search', rs.cv_results_)\n",
    "\n",
    "\n",
    "    def run_bayes_search(self):\n",
    "        bs = BayesSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state, n_jobs=self.n_jobs)\n",
    "        bs.fit(self.X_train, self.y_train)\n",
    "        score = bs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('bayes_search', bs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def run_all():\n",
    "    algorithms = [\n",
    "        ('random_forest', RandomForestClassifier()),\n",
    "        ('neural_network', MLPClassifier()),\n",
    "        ('gradient_boosting', GradientBoostingClassifier())\n",
    "    ]\n",
    "\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        hpo = HPO(algorithm[1], search_spaces[algorithm[0]], n_iter=500, n_jobs=-1)\n",
    "        for data_set in data_sets:\n",
    "            hpo.load_dataset(data_sets[data_set])\n",
    "            hpo.run_random_search()\n",
    "            hpo.run_bayes_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hpo = HPO(GradientBoostingClassifier(), search_spaces['gradient_boosting'], n_iter = 500, n_jobs=-1)\n",
    "hpo.load_dataset(data_sets['spambase'])\n",
    "hpo.run_bayes_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "algorithms = [\n",
    "    \"GradientBoostingClassifier\",\n",
    "    \"MLPClassifier\",\n",
    "    \"RandomForestClassifier\"\n",
    "]\n",
    "\n",
    "optimalization_method = [\n",
    "    \"bayes_search\",\n",
    "    \"random_search\"\n",
    "]\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "data = {}\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    data[algorithm] = {}\n",
    "    for method in optimalization_method:\n",
    "        file_pattern = f'{method}-*-{algorithm}-0.csv'\n",
    "        file_paths = glob.glob(os.path.join(data_folder, file_pattern))\n",
    "        data_frames = []\n",
    "        for file_path in file_paths:\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            df = df.iloc[:, :-1]\n",
    "            data_frames.append(df)\n",
    "\n",
    "        data[algorithm][method] = data_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "best_configuration_index_per_algorithm = {}\n",
    "\n",
    "for algorithm in data.keys():\n",
    "    data_sets = data[algorithm]['random_search']\n",
    "    first_data_set = data_sets[0]\n",
    "\n",
    "    for i in range(1, len(data_sets)):\n",
    "        other_data_set = data_sets[i]\n",
    "        last_column = other_data_set.iloc[:, -1]\n",
    "        first_data_set = pd.concat([first_data_set, last_column], axis=1)\n",
    "\n",
    "    last_columns = first_data_set.iloc[:, -len(data_sets):]\n",
    "    max_avg_index = last_columns.mean(axis=1).idxmax()\n",
    "    best_configuration = first_data_set.loc[max_avg_index]\n",
    "    best_configuration_index_per_algorithm[algorithm] = max_avg_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in data.keys():\n",
    "    for method in data[algorithm].keys():\n",
    "        data_sets = data[algorithm][method]\n",
    "\n",
    "        for i in range(len(data_sets)):\n",
    "            diffs = data_sets[i]['Srednia dokladnosc'][best_configuration_index_per_algorithm[algorithm]] - data_sets[i]['Srednia dokladnosc']\n",
    "            print(algorithm, method)\n",
    "            print(diffs)\n",
    "            data_sets[i]['Differents'] = diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for algorithm in data.keys():\n",
    "    for method in data[algorithm].keys():\n",
    "        data_sets = data[algorithm][method]\n",
    "\n",
    "        for i in range(len(data_sets)):\n",
    "            plt.figure()\n",
    "            plt.boxplot(data_sets[i]['Differents'])\n",
    "            plt.xlabel(algorithm)\n",
    "            plt.ylabel('Difference')\n",
    "            plt.title(f'{algorithm} - {method}')\n",
    "            plt.savefig(f'plots/{algorithm}-{method}-{i}.svg')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for algorithm in data.keys():\n",
    "    data_sets = data[algorithm]['bayes_search']\n",
    "\n",
    "    for i in range(len(data_sets)):\n",
    "        current_data_set = data_sets[i]\n",
    "\n",
    "        current_best = 0\n",
    "        current_index = 0\n",
    "        y = []\n",
    "        x = []\n",
    "\n",
    "        for value in current_data_set['Srednia dokladnosc']:\n",
    "            current_index += 1\n",
    "            if(abs(value -  current_best) < 0.005 * current_best or value > current_best):\n",
    "                y.append(value)\n",
    "                x.append(current_index)\n",
    "\n",
    "            if(value > current_best):\n",
    "                current_best = value\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(x, y, s=10)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('AUX')\n",
    "        plt.title(f'{algorithm} - Dataset {i+1}')\n",
    "        plt.savefig(f'scatters/{algorithm}-{i}.svg')\n",
    "        plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
