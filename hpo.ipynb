{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane zbiory danych: \n",
    "- Satellite https://www.openml.org/search?type=data&status=active&id=40900\n",
    "- bank-marketing https://www.openml.org/search?type=data&status=active&id=1558\n",
    "- spambase https://www.openml.org/search?type=data&status=active&id=44\n",
    "- phoneme https://www.openml.org/search?type=data&status=active&id=1489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane algorytmy:\n",
    "* GradientBoosting\n",
    "* RandomForest\n",
    "* Sieci neuronowe\n",
    "\n",
    "W przypadku sieci neuronowych startową siatkę hiperparametrów zaczerpnięto z artykułu https://www.degruyter.com/document/doi/10.1515/comp-2020-0227/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "1. Sprawdzić dane - done\n",
    "2. Uporządkować metody w ramach jednej klasy - done\n",
    "3. Zapisywanie wyników - done\n",
    "4. Wybranie siatek hiperparamterów - done\n",
    "5. Obliczanie tuningu \n",
    "6. Wykresiki i docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_sets = {\n",
    "    # 5.1k\n",
    "    'satellite' : {\n",
    "        'id' : 40900,\n",
    "        'label_y' : 'target'\n",
    "    },\n",
    "    # 4.52\n",
    "    'bank-marketing' : {\n",
    "        'id' : 1558,\n",
    "        'label_y' : 'class'\n",
    "    },\n",
    "    # 5.4k\n",
    "    'phoneme' : {\n",
    "        'id' : 1489,\n",
    "        'label_y' : 'class'\n",
    "    },\n",
    "    # 4.6k\n",
    "    'spambase' : {\n",
    "        'id' : 44,\n",
    "        'label_y' : 'class'\n",
    "    },\n",
    "}\n",
    "\n",
    "search_spaces = {\n",
    "    'random_forest' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 2000),\n",
    "        \"estimator__max_depth\" : np.arange(1, 20),\n",
    "        \"estimator__max_samples\" : np.linspace(0.1, 1, num=100),\n",
    "        \"estimator__max_features\" : ['sqrt', 'log2'],\n",
    "        \"estimator__min_samples_split\" :  np.linspace(0.1, 1, num=100),\n",
    "    },\n",
    "    'neural_network' : {\n",
    "        \"estimator__hidden_layer_sizes\" : np.arange(10, 500),\n",
    "        \"estimator__activation\" : ['relu', 'identity', 'logistic', 'tanh'],\n",
    "        \"estimator__learning_rate_init\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "        \"estimator__alpha\" : np.logspace(-6, 1, base=2.0, num=1000),\n",
    "    },\n",
    "    'gradient_boosting' : {\n",
    "        \"estimator__n_estimators\" : np.arange(1, 5000),\n",
    "        \"estimator__learning_rate\" : np.logspace(-10, 0, base=2.0, num=1000),\n",
    "        \"estimator__subsample\" : np.linspace(0.1, 1, num=100),\n",
    "        \"estimator__loss\" : ['log_loss', 'exponential'],\n",
    "        \"estimator__max_depth\" : np.arange(1, 15),\n",
    "        \"estimator__min_samples_split\" : np.logspace(0, 7, base=2.0, num=1000),\n",
    "        \"estimator__max_features\" : np.arange(0, 1, 0.05),\n",
    "        \"estimator__reg_lambda\" : np.logspace(-10, 10, base=2.0, num=1000),\n",
    "        \"estimator__reg_alpha\" : np.logspace(-10, 10, base=2.0, num=1000),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "class HPO():\n",
    "    def __init__(self, estimator, search_space, random_state=0, test_size=0.2, n_iter=10): \n",
    "        num_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='mean')),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ])\n",
    "\n",
    "        cat_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ])\n",
    "\n",
    "        preprocessing = ColumnTransformer(transformers=[\n",
    "            ('num_pipeline',num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "            ('cat_pipeline',cat_pipeline, make_column_selector(dtype_include=np.object_))\n",
    "            ],\n",
    "            remainder='drop',\n",
    "            n_jobs=-1)\n",
    "\n",
    "        self.pipeline = Pipeline(steps=[\n",
    "            ('preprocessing', preprocessing),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.search_space = search_space\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.estimator_name = estimator.__class__.__name__\n",
    "\n",
    "    def load_dataset(self, data_set):\n",
    "        dataset = openml.datasets.get_dataset(data_set['id'])\n",
    "        self.dataset_id = data_set['id']\n",
    "\n",
    "        X, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
    "\n",
    "        y = X.iloc[:,-1]\n",
    "        X = X.iloc[:,:-1]\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "\n",
    "    def save_to_file(self, type, results):\n",
    "        csv_file_path = '{0}-{1}-{2}-{3}.csv'.format(type, self.dataset_id, self.estimator_name, self.random_state)\n",
    "\n",
    "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['Iteracja', 'Parametry', 'Średnia dokładność', 'Czas']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            for i in range(len(results['params'])):\n",
    "                writer.writerow({\n",
    "                    'Iteracja': i + 1,\n",
    "                    'Parametry': str(results['params'][i]),\n",
    "                    'Średnia dokładność': results['mean_test_score'][i],\n",
    "                    'Czas': results['mean_fit_time'][i]\n",
    "                })\n",
    "\n",
    "        print(f\"Wyniki zostały zapisane do pliku CSV: {csv_file_path}\")\n",
    "\n",
    "    def run_random_search(self):\n",
    "        rs = RandomizedSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state)\n",
    "        rs.fit(self.X_train, self.y_train)\n",
    "        score = rs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('random_search', rs.cv_results_)\n",
    "\n",
    "\n",
    "    def run_bayes_search(self):\n",
    "        bs = BayesSearchCV(self.pipeline, self.search_space, n_iter=self.n_iter, random_state=self.random_state)\n",
    "        bs.fit(self.X_train, self.y_train)\n",
    "        score = bs.score(self.X_test, self.y_test)\n",
    "        print(score)\n",
    "        self.save_to_file('bayes_search', bs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def run_all():\n",
    "    algorithms = [\n",
    "        ('random_forest', RandomForestClassifier()),\n",
    "        ('neural_network', MLPClassifier()),\n",
    "        ('gradient_boosting', GradientBoostingClassifier())\n",
    "    ]\n",
    "\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        hpo = HPO(algorithm[1], search_spaces[algorithm[0]], n_iter=500)\n",
    "        for data_set in data_sets:\n",
    "            hpo.load_dataset(data_sets[data_set])\n",
    "            hpo.run_random_search()\n",
    "            hpo.run_bayes_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9456953642384106\n",
      "Wyniki zostały zapisane do pliku CSV: random_search-38-RandomForestClassifier-0.csv\n",
      "0.9615894039735099\n",
      "Wyniki zostały zapisane do pliku CSV: bayes_search-38-RandomForestClassifier-0.csv\n",
      "0.8406921241050119\n",
      "Wyniki zostały zapisane do pliku CSV: random_search-40536-RandomForestClassifier-0.csv\n"
     ]
    }
   ],
   "source": [
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882352941176471\n",
      "Wyniki zostały zapisane do pliku CSV: random_search-40900-RandomForestClassifier-0.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hpo = HPO(RandomForestClassifier(), search_spaces['random_forest'], n_iter = 10)\n",
    "hpo.load_dataset(data_sets['satellite'])\n",
    "hpo.run_random_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
